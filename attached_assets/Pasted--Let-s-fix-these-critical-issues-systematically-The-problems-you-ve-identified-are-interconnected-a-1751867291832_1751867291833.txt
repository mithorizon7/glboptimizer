 Let's fix these critical issues systematically. The problems you've identified are interconnected and relate to how the application is configured, initialized, and how its different components (Flask, Celery, Database) interact.

Here is a step-by-step guide to resolve these issues.

1. Fix Missing Celery Imports and Initialization

The error "celery" is not defined occurs because the main application file (app.py) and the cleanup scheduler (cleanup_scheduler.py) are trying to use the Celery instance before it has been properly created. The project was refactored to use a "factory pattern" to create the Celery app, which is a good practice, but the files using it weren't all updated.

Your new app-2.py file already contains the correct fix for the main application.

Solution:

Confirm the Fix in app-2.py: Your new file correctly imports the factory function and creates the Celery instance. This is the correct approach.

Python
// In app-2.py
from celery_app import make_celery

# ...

# Initialize Celery app
celery = make_celery(__name__)
Apply the Same Fix to the Cleanup Scheduler: The cleanup_scheduler.py file has the same bug. It needs to be updated to create its own Celery instance.

Open cleanup_scheduler.py and replace the line from celery_app import celery with the factory pattern:

Python
// In cleanup_scheduler.py

# OLD CODE (Incorrect)
# from celery_app import celery

# NEW CODE (Correct)
from celery_app import make_celery
celery = make_celery() 

# ... (rest of the file)
This change ensures that any part of your application that needs to interact with the task queue creates its own properly configured Celery instance, resolving the not defined errors.

2. Fix Environment Variable and Startup Failures

Workflow startup failures are often caused by missing environment variables. The application crashes because a required setting (like the database URL) isn't available when a module is imported. The best way to fix this is to ensure that sane defaults are set at the very beginning of the application's entry point.

Solution:

The startup scripts (develop.py and run_production.py) are the best place to centralize and validate the environment.

Open develop.py and add default settings at the top. This ensures the app can always start in a development environment without needing a .env file.

Python
# In develop.py

import os

# Set sane defaults BEFORE any other application imports
os.environ.setdefault('FLASK_ENV', 'development')
os.environ.setdefault('DATABASE_URL', 'sqlite:///dev.db') // Use a simple SQLite DB for development
os.environ.setdefault('REDIS_URL', 'redis://localhost:6379/0')
os.environ.setdefault('CELERY_BROKER_URL', os.environ['REDIS_URL'])
os.environ.setdefault('CELERY_RESULT_BACKEND', os.environ['REDIS_URL'])
os.environ.setdefault('SESSION_SECRET', 'dev_secret_key_for_local_development_only')

# ... (rest of the script)
Apply a Similar Change to run_production.py: For production, you should check that critical variables are set and fail loudly if they are not. Your security_audit.py script already performs these checks, which is excellent.

By setting these defaults, you make the application more resilient and easier to run, especially during development, preventing crashes due to misconfiguration.

3. Fix Task Import and Method Access Errors

The error object has no attribute 'optimizer' found in your test results points to a flaw in how the pipeline tasks are structured. The Celery tasks in pipeline_tasks.py were not correctly using the GLBOptimizer instance.

The file pipeline_improvements.py contains the correctly designed, more robust logic for handling pipeline stages. The best solution is to replace the faulty logic in pipeline_tasks.py with this improved structure.

Solution:

This is the most critical fix. You need to refactor pipeline_tasks.py to use the more resilient patterns found in pipeline_improvements.py.

Open pipeline_tasks.py and replace its content with a structure that properly initializes and uses the optimizer within each stage.

Python
# In pipeline_tasks.py (this is a corrected and simplified version based on your files)

import os
import logging
from celery_app import make_celery
from optimizer import GLBOptimizer
from database import SessionLocal
from models import OptimizationTask
from sqlalchemy import text
from datetime import datetime, timezone

logger = logging.getLogger(__name__)
celery_app = make_celery('glb_pipeline')

class PipelineStage:
    """
    An enhanced, self-contained pipeline stage that handles its own
    optimizer instance, progress updates, and error logging.
    """
    def __init__(self, task_id: str, stage_name: str):
        self.task_id = task_id
        self.stage_name = stage_name
        # Each stage gets its own optimizer instance
        self.optimizer = GLBOptimizer()

    def update_progress(self, progress: int, message: str, status: str = 'processing'):
        # This method now correctly updates the database
        try:
            db = SessionLocal()
            query = text("""
                UPDATE optimization_tasks 
                SET progress = :progress, current_step = :step, status = :status, updated_at = :updated_at
                WHERE id = :task_id
            """)
            db.execute(query, {
                'progress': progress,
                'step': f"{self.stage_name}: {message}",
                'status': status,
                'updated_at': datetime.now(timezone.utc),
                'task_id': self.task_id
            })
            db.commit()
        except Exception as e:
            logger.error(f"Failed to update progress for task {self.task_id}: {e}")
        finally:
            db.close()

@celery_app.task(bind=True, name='pipeline.prune_model')
def prune_model_task(self, task_id: str, input_path: str, output_path: str, model_info: dict):
    """
    Stage 2: Prune unused data (Example of a corrected task)
    """
    stage = PipelineStage(task_id, "Data Cleanup")
    stage.update_progress(15, "Removing unused data")

    try:
        prune_output = input_path.replace('.glb', '_pruned.glb')

        # This call is now guaranteed to work because the `optimizer`
        # is an attribute of the `stage` object.
        result = stage.optimizer._run_gltf_transform_prune(input_path, prune_output)

        if result['success']:
            stage.update_progress(25, "Cleanup complete")
            # Chain to the next task...
            weld_model_task.delay(task_id, prune_output, output_path, model_info)
            return {'success': True}
        else:
            raise Exception(result.get('error', 'Pruning failed'))

    except Exception as e:
        logger.error(f"Pruning failed for task {task_id}: {e}")
        stage.update_progress(15, f"Cleanup failed: {e}", status='failed')
        return {'success': False, 'error': str(e)}

# (Apply this pattern to all other tasks in pipeline_tasks.py: weld, compress, etc.)
By making these three systematic changes, you will resolve the critical errors and make the application stable and ready for use.